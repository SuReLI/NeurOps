{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial walks through some uses of the PyTorch implementation of NeurOps.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the PyTorch implementation and some other useful packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch.src import *\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a LeNet-style model, which has three convolutional model followed by two fully-connected model. We use the `ModSequential` class to wrap the `ModConv2d` and `ModLinear` model, which allows us to mask, prune, and grow the model. We also use the `track_activations` and `track_auxiliary_gradients` arguments to enable the tracking of activations and auxiliary gradients later. By adding the `input_shape` of the data, we can compute the conversion factor of how many input neurons to add to the first linear layer when a new output channel is added to the final convolutional layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 15538 effective parameters.\n",
      "The conversion factor of this model is 4 after layer 2.\n"
     ]
    }
   ],
   "source": [
    "model = ModSequential(\n",
    "        ModConv2d(in_channels=1, out_channels=8, kernel_size=7, masked=True, padding=1, learnable_mask=True),\n",
    "        ModConv2d(in_channels=8, out_channels=16, kernel_size=7, masked=True, padding=1, prebatchnorm=True, learnable_mask=True),\n",
    "        ModConv2d(in_channels=16, out_channels=16, kernel_size=5, masked=True, prebatchnorm=True, learnable_mask=True),\n",
    "        ModLinear(64, 32, masked=True, learnable_mask=True),\n",
    "        ModLinear(32, 10, masked=True),\n",
    "        track_activations=True,\n",
    "        track_auxiliary_gradients=True,\n",
    "        input_shape = (1, 14, 14)\n",
    "    ).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"This model has {} effective parameters.\".format(model.parameter_count(masked = True)))\n",
    "print(\"The conversion factor of this model is {} after layer {}.\".format(model.conversion_factor, model.conversion_layer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a dataset and define standard training and testing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                     transform=transforms.Compose([ \n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                            transforms.Resize((14,14))\n",
    "                        ]))\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, lengths=[int(0.9*len(dataset)), int(0.1*len(dataset))])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                            transforms.Resize((14,14))\n",
    "                        ])),\n",
    "    batch_size=128, shuffle=True)\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, epochs=10, val_loader=None):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "        if val_loader is not None:\n",
    "            print(\"Validation: \", end = \"\")\n",
    "            test(model, val_loader, criterion)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pretrain the model before changing its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 2.310916\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 2.199961\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 1.728310\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 1.394315\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 1.246807\n",
      "Validation: Average loss: 0.0099, Accuracy: 3391/6000 (56.52%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 1.293340\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 1.089069\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.802971\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.834708\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.709585\n",
      "Validation: Average loss: 0.0068, Accuracy: 4036/6000 (67.27%)\n",
      "Train Epoch: 2 [0/54000 (0%)]\tLoss: 0.682295\n",
      "Train Epoch: 2 [12800/54000 (24%)]\tLoss: 0.808546\n",
      "Train Epoch: 2 [25600/54000 (47%)]\tLoss: 0.623847\n",
      "Train Epoch: 2 [38400/54000 (71%)]\tLoss: 0.658601\n",
      "Train Epoch: 2 [51200/54000 (95%)]\tLoss: 0.809328\n",
      "Validation: Average loss: 0.0049, Accuracy: 4580/6000 (76.33%)\n",
      "Train Epoch: 3 [0/54000 (0%)]\tLoss: 0.618186\n",
      "Train Epoch: 3 [12800/54000 (24%)]\tLoss: 0.490364\n",
      "Train Epoch: 3 [25600/54000 (47%)]\tLoss: 0.424031\n",
      "Train Epoch: 3 [38400/54000 (71%)]\tLoss: 0.462391\n",
      "Train Epoch: 3 [51200/54000 (95%)]\tLoss: 0.402286\n",
      "Validation: Average loss: 0.0029, Accuracy: 5156/6000 (85.93%)\n",
      "Train Epoch: 4 [0/54000 (0%)]\tLoss: 0.400611\n",
      "Train Epoch: 4 [12800/54000 (24%)]\tLoss: 0.309207\n",
      "Train Epoch: 4 [25600/54000 (47%)]\tLoss: 0.217848\n",
      "Train Epoch: 4 [38400/54000 (71%)]\tLoss: 0.248164\n",
      "Train Epoch: 4 [51200/54000 (95%)]\tLoss: 0.344702\n",
      "Validation: Average loss: 0.0027, Accuracy: 5200/6000 (86.67%)\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, optimizer, criterion, epochs=5, val_loader=val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use NeurOps to optimize the model.\n",
    "\n",
    "First, we use a heuristic from `metrics.py` to measure the existing channels and neurons to determine which ones to prune. The simplest one is measuring the norm of incoming weights to a neuron. We'll copy the model (so we have access to the original), then score each neuron and prune the lowest scoring ones within each layer. After running the following block, try uncommenting different lines to see how different metrics affect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 scores: mean 4.01, std 0.291, min 3.7, smallest 25%: [6 2]\n",
      "Layer 1 scores: mean 10.9, std 0.592, min 10.2, smallest 25%: [ 4  5  9 10]\n",
      "Layer 2 scores: mean 11.9, std 0.921, min 10.6, smallest 25%: [ 2  5  8 10]\n",
      "Layer 3 scores: mean 4.44, std 0.42, min 3.62, smallest 25%: [27 24 30 19 14  1  9 26]\n",
      "The pruned model has 8914 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0079, Accuracy: 3952/6000 (65.87%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 1.396421\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.634928\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.366687\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.492258\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.281330\n",
      "Validation: Average loss: 0.0030, Accuracy: 5180/6000 (86.33%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.261281\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.302074\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.503342\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.276111\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.392927\n",
      "Validation: Average loss: 0.0028, Accuracy: 5185/6000 (86.42%)\n"
     ]
    }
   ],
   "source": [
    "modded_model = copy.deepcopy(model)\n",
    "modded_optimizer = torch.optim.SGD(modded_model.parameters(), lr=0.01)\n",
    "modded_optimizer.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for i in range(len(model)-1):\n",
    "    scores = weight_sum(model[i].weight)\n",
    "    # scores = weight_sum(model[i].weight) +  weight_sum(model[i+1].weight, fanin=False, conversion_factor=model.conversion_factor if i == model.conversion_layer else -1)\n",
    "    # scores = activation_variance(model.activations[str(i)])\n",
    "    # scores = svd_score(model.activations[str(i)])\n",
    "    # scores = nuclear_score(model.activations[str(i)], average=i<3)\n",
    "    # Before trying this line, run the following block: # scores = fisher_info(mask_grads[i])\n",
    "    print(\"Layer {} scores: mean {:.3g}, std {:.3g}, min {:.3g}, smallest 25%:\".format(i, scores.mean(), scores.std(), scores.min()), end=\" \")\n",
    "    to_prune = np.argsort(scores.detach().numpy())[:int(0.25*len(scores))]\n",
    "    print(to_prune)\n",
    "    modded_model.prune(i, to_prune, optimizer=modded_optimizer, clear_activations=True)\n",
    "print(\"The pruned model has {} effective parameters.\".format(modded_model.parameter_count(masked = True)))\n",
    "print(\"Validation after pruning: \", end = \"\")\n",
    "test(modded_model, val_loader, criterion)\n",
    "train(modded_model, train_loader, modded_optimizer, criterion, epochs=2, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_mask_grads():\n",
    "    mask_grads = []\n",
    "    for i in range(len(model.activations)-1):\n",
    "        mask_grads.append(torch.empty(0, *model[i].mask_vector.shape))\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        for i in range(len(model.activations)-1):\n",
    "            mask_grads[i] = torch.cat([mask_grads[i], model[i].mask_vector.grad.unsqueeze(0)])\n",
    "    return mask_grads\n",
    "\n",
    "#mask_grads = collect_mask_grads()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try iterative pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 scores: mean 4.01, std 0.291, min 3.7, smallest 15%: [6]\n",
      "Layer 1 scores: mean 9.6, std 0.599, min 8.88, smallest 15%: [5 4]\n",
      "Layer 2 scores: mean 10.5, std 0.856, min 9.25, smallest 15%: [5 2]\n",
      "Layer 3 scores: mean 3.95, std 0.383, min 3.3, smallest 15%: [27 19 24  1]\n",
      "The pruned model now has 12008 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0037, Accuracy: 4966/6000 (82.77%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 1.065550\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.311489\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.347963\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.363400\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.316475\n",
      "Validation: Average loss: 0.0028, Accuracy: 5206/6000 (86.77%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.255610\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.179851\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.273494\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.214253\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.369089\n",
      "Validation: Average loss: 0.0027, Accuracy: 5215/6000 (86.92%)\n",
      "Layer 0 scores: mean 4.15, std 0.266, min 3.89, smallest 15%: [2]\n",
      "Layer 1 scores: mean 8.51, std 0.472, min 8.1, smallest 15%: [13 12]\n",
      "Layer 2 scores: mean 10.3, std 0.953, min 8.86, smallest 15%: [5 6]\n",
      "Layer 3 scores: mean 3.73, std 0.34, min 3.15, smallest 15%: [ 2 13  8 15]\n",
      "The pruned model now has 8914 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0050, Accuracy: 4610/6000 (76.83%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.689406\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.312419\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.443479\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.475497\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.251923\n",
      "Validation: Average loss: 0.0027, Accuracy: 5214/6000 (86.90%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.315284\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.311347\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.346029\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.364356\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.265780\n",
      "Validation: Average loss: 0.0026, Accuracy: 5225/6000 (87.08%)\n",
      "Layer 0 scores: mean 4.29, std 0.24, min 4.08, smallest 15%: []\n",
      "Layer 1 scores: mean 8.82, std 0.55, min 8.3, smallest 15%: [8]\n",
      "Layer 2 scores: mean 10.4, std 1, min 8.59, smallest 15%: [6]\n",
      "Layer 3 scores: mean 3.68, std 0.298, min 3.18, smallest 15%: [ 3 13 20]\n",
      "The pruned model now has 7780 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0034, Accuracy: 5024/6000 (83.73%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.458603\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.277080\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.328006\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.347583\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.376551\n",
      "Validation: Average loss: 0.0026, Accuracy: 5231/6000 (87.18%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.442268\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.237986\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.385638\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.377946\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.251457\n",
      "Validation: Average loss: 0.0025, Accuracy: 5236/6000 (87.27%)\n",
      "Layer 0 scores: mean 4.36, std 0.212, min 4.18, smallest 15%: []\n",
      "Layer 1 scores: mean 9.09, std 0.583, min 8.61, smallest 15%: [6]\n",
      "Layer 2 scores: mean 10, std 0.869, min 7.94, smallest 15%: [6]\n",
      "Layer 3 scores: mean 3.54, std 0.284, min 3.11, smallest 15%: [10 11 13]\n",
      "The pruned model now has 6720 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0038, Accuracy: 4851/6000 (80.85%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.372106\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.313874\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.311888\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.319379\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.299428\n",
      "Validation: Average loss: 0.0026, Accuracy: 5237/6000 (87.28%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.306606\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.405268\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.341106\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.237019\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.266737\n",
      "Validation: Average loss: 0.0025, Accuracy: 5235/6000 (87.25%)\n",
      "Layer 0 scores: mean 4.42, std 0.193, min 4.24, smallest 15%: []\n",
      "Layer 1 scores: mean 9.38, std 0.637, min 8.86, smallest 15%: [9]\n",
      "Layer 2 scores: mean 9.6, std 0.649, min 8.4, smallest 15%: [7]\n",
      "Layer 3 scores: mean 3.43, std 0.263, min 2.96, smallest 15%: [ 7 16]\n",
      "The pruned model now has 5781 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0039, Accuracy: 4932/6000 (82.20%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.417025\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.345414\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.311447\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.281655\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.422159\n",
      "Validation: Average loss: 0.0026, Accuracy: 5222/6000 (87.03%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.229719\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.241673\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.395002\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.307319\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.219365\n",
      "Validation: Average loss: 0.0025, Accuracy: 5230/6000 (87.17%)\n"
     ]
    }
   ],
   "source": [
    "modded_model_iterative = copy.deepcopy(model)\n",
    "modded_optimizer_iterative = torch.optim.SGD(modded_model_iterative.parameters(), lr=0.01)\n",
    "modded_optimizer_iterative.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for iter in range(5):\n",
    "    for i in range(len(modded_model_iterative)-1):\n",
    "        scores = weight_sum(modded_model_iterative[i].weight)\n",
    "        # scores = weight_sum(model[i].weight) +  weight_sum(model[i+1].weight, fanin=False, conversion_factor=model.conversion_factor if i == model.conversion_layer else -1)\n",
    "        # scores = activation_variance(model.activations[str(i)])\n",
    "        # scores = svd_score(model.activations[str(i)])\n",
    "        # scores = nuclear_score(model.activations[str(i)], average=i<3)\n",
    "        print(\"Layer {} scores: mean {:.3g}, std {:.3g}, min {:.3g}, smallest 15%:\".format(i, scores.mean(), scores.std(), scores.min()), end=\" \")\n",
    "        to_prune = np.argsort(scores.detach().numpy())[:int(0.15*len(scores))]\n",
    "        print(to_prune)\n",
    "        modded_model_iterative.prune(i, to_prune, optimizer=modded_optimizer_iterative, clear_activations=True)\n",
    "    print(\"The pruned model now has {} effective parameters.\".format(modded_model_iterative.parameter_count(masked = True)))\n",
    "    print(\"Validation after pruning: \", end = \"\")\n",
    "    test(modded_model_iterative, val_loader, criterion)\n",
    "    train(modded_model_iterative, train_loader, modded_optimizer_iterative, criterion, epochs=2, val_loader=val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can also grow the model. The following cell uses a neurogenesis strategy similar to NORTH-Random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 score: 8/8, neurons to add: 1\n",
      "Layer 1 score: 16/16, neurons to add: 1\n",
      "Layer 2 score: 16/16, neurons to add: 1\n",
      "Layer 3 score: 32/32, neurons to add: 2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[34, 68]' is invalid for input of size 2176",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     to_add \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(score\u001b[39m-\u001b[39m\u001b[39mint\u001b[39m(\u001b[39m0.95\u001b[39m\u001b[39m*\u001b[39mmax_rank), \u001b[39m0\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLayer \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m score: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, neurons to add: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i, score, max_rank, to_add))\n\u001b[0;32m---> 12\u001b[0m     modded_model_grow\u001b[39m.\u001b[39;49mgrow(i, to_add, fanin_weights\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39miterative_orthogonalization\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     13\u001b[0m                            optimizer\u001b[39m=\u001b[39;49mmodded_optimizer_grow)\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe grown model now has \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m effective parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(modded_model_grow\u001b[39m.\u001b[39mparameter_count(masked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)))\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mValidation after growing: \u001b[39m\u001b[39m\"\u001b[39m, end \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/NeurOps/pytorch/src/models.py:146\u001b[0m, in \u001b[0;36mModSequential.grow\u001b[0;34m(self, layer_index, newneurons, fanin_weights, fanout_weights, optimizer, clear_activations, send_activations)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mfor\u001b[39;00m i, module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m layer_index \u001b[39mand\u001b[39;00m (\u001b[39misinstance\u001b[39m(module, ModLinear) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(module, ModConv2d)):\n\u001b[0;32m--> 146\u001b[0m         module\u001b[39m.\u001b[39;49mgrow(newneurons, \u001b[39m0\u001b[39;49m, fanin_weights \u001b[39m=\u001b[39;49m fanin_weights, optimizer\u001b[39m=\u001b[39;49moptimizer, \n\u001b[1;32m    147\u001b[0m                     activations\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivations[\u001b[39mstr\u001b[39;49m(layer_index\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)] \u001b[39mif\u001b[39;49;00m send_activations \u001b[39mor\u001b[39;49;00m fanin_weights \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39miterative_orthogonalization\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    148\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_activations:\n\u001b[1;32m    149\u001b[0m             \u001b[39mif\u001b[39;00m clear_activations:\n",
      "File \u001b[0;32m~/repos/NeurOps/pytorch/src/layers.py:313\u001b[0m, in \u001b[0;36mModLinear.grow\u001b[0;34m(self, new_out_features, new_in_features, fanin_weights, fanout_weights, optimizer, activations)\u001b[0m\n\u001b[1;32m    311\u001b[0m     fanin_weights \u001b[39m=\u001b[39m kaiming_uniform(torch\u001b[39m.\u001b[39mzeros(new_out_features\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features))[:new_out_features, :]\n\u001b[1;32m    312\u001b[0m \u001b[39melif\u001b[39;00m fanin_weights \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miterative_orthogonalization\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 313\u001b[0m     fanin_weights \u001b[39m=\u001b[39m iterative_orthogonalization(torch\u001b[39m.\u001b[39;49mzeros(new_out_features\u001b[39m+\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_features, \n\u001b[1;32m    314\u001b[0m                                                            \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_features), \n\u001b[1;32m    315\u001b[0m                                                \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mactivations)[:new_out_features, :]\n\u001b[1;32m    316\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(fanin_weights, torch\u001b[39m.\u001b[39mTensor) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(fanin_weights\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    317\u001b[0m     fanin_weights \u001b[39m=\u001b[39m fanin_weights\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/repos/NeurOps/pytorch/src/initializations.py:21\u001b[0m, in \u001b[0;36miterative_orthogonalization\u001b[0;34m(weights, input, stride, output_normalize)\u001b[0m\n\u001b[1;32m     19\u001b[0m numneurons \u001b[39m=\u001b[39m weights\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     20\u001b[0m u, s, v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msvd(\u001b[39minput\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m weights \u001b[39m=\u001b[39m (u[:numneurons,:numneurons]\u001b[39m.\u001b[39;49mmm(torch\u001b[39m.\u001b[39;49mdiag(\u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49mtorch\u001b[39m.\u001b[39;49msqrt(s[:numneurons])))\u001b[39m.\u001b[39;49mmm(v[:,:numneurons]\u001b[39m.\u001b[39;49mt()))\u001b[39m.\u001b[39;49mreshape(numneurons, \u001b[39m*\u001b[39;49mweights\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m:])\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m output_normalize:\n\u001b[1;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(weights\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[34, 68]' is invalid for input of size 2176"
     ]
    }
   ],
   "source": [
    "modded_model_grow = copy.deepcopy(model)\n",
    "modded_optimizer_grow = torch.optim.SGD(modded_model_grow.parameters(), lr=0.01)\n",
    "modded_optimizer_grow.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for iter in range(5):\n",
    "    for i in range(len(modded_model_grow)-1):\n",
    "        #score = orthogonality_gap(modded_model_grow.activations[str(i)])\n",
    "        max_rank = modded_model_grow[i].out_features if i > modded_model_grow.conversion_layer else modded_model_grow[i].out_channels\n",
    "        score = effective_rank(modded_model_grow.activations[str(i)])\n",
    "        to_add = max(score-int(0.95*max_rank), 0)\n",
    "        print(\"Layer {} score: {}/{}, neurons to add: {}\".format(i, score, max_rank, to_add))\n",
    "        modded_model_grow.grow(i, to_add, fanin_weights=\"iterative_orthogonalization\", \n",
    "                               optimizer=modded_optimizer_grow)\n",
    "    print(\"The grown model now has {} effective parameters.\".format(modded_model_grow.parameter_count(masked = True)))\n",
    "    print(\"Validation after growing: \", end = \"\")\n",
    "    test(modded_model_grow, val_loader, criterion)\n",
    "    train(modded_model_grow, train_loader, modded_optimizer_grow, criterion, epochs=2, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try masking neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modded_model_masked = copy.deepcopy(model)\n",
    "modded_optimizer_masked = torch.optim.SGD(modded_model_masked.parameters(), lr=0.01)\n",
    "modded_optimizer_masked.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for i in range(len(modded_model_masked)-1):\n",
    "    neurons = modded_model_masked[i].out_features if i > modded_model_masked.conversion_layer else modded_model_masked[i].out_channels\n",
    "    modded_model_masked.mask(i, list(range(neurons//2, neurons)))\n",
    "\n",
    "for iter in range(1):\n",
    "    for i in range(len(modded_model_masked)-1):\n",
    "        score = effective_rank(modded_model_masked.activations[str(i)])\n",
    "        to_add = max(score-int(0.95*max_rank), 0)\n",
    "        print(\"Layer {} score: {}/{}, neurons to add: {}\".format(i, score, max_rank, to_add))\n",
    "        modded_model_masked.grow(i, to_add, fanin_weights=\"iterative_orthogonalization\", \n",
    "                                 optimizer=modded_optimizer_masked, mask=True)\n",
    "    print(\"The grown model now has {} effective parameters.\".format(modded_model_masked.parameter_count(masked = True)))\n",
    "    print(\"Validation after growing: \", end = \"\")\n",
    "    test(modded_model_masked, val_loader, criterion)\n",
    "    train(modded_model_masked, train_loader, modded_optimizer_masked, criterion, epochs=2, val_loader=val_loader)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad3e4a8528e73303fbef1750ea88a8047a0bb395378c7b9a503e4faa36c1cedd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
