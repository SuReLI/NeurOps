{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial walks through some uses of the PyTorch implementation of NeurOps.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the PyTorch implementation and some other useful packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch.neurops import *\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a LeNet-style model, which has three convolutional model followed by two fully-connected model. We use the `ModSequential` class to wrap the `ModConv2d` and `ModLinear` model, which allows us to mask, prune, and grow the model. We also use the `track_activations` and `track_auxiliary_gradients` arguments to enable the tracking of activations and auxiliary gradients later. By adding the `input_shape` of the data, we can compute the conversion factor of how many input neurons to add to the first linear layer when a new output channel is added to the final convolutional layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModSequential(\n",
    "        ModConv2d(in_channels=1, out_channels=8, kernel_size=7, masked=True, padding=1, learnable_mask=True),\n",
    "        ModConv2d(in_channels=8, out_channels=16, kernel_size=7, masked=True, padding=1, prebatchnorm=True, learnable_mask=True),\n",
    "        ModConv2d(in_channels=16, out_channels=16, kernel_size=5, masked=True, prebatchnorm=True, learnable_mask=True),\n",
    "        ModLinear(64, 32, masked=True, prebatchnorm=True, learnable_mask=True),\n",
    "        ModLinear(32, 10, masked=True, prebatchnorm=True, nonlinearity=\"\"),\n",
    "        track_activations=True,\n",
    "        track_auxiliary_gradients=True,\n",
    "        input_shape = (1, 14, 14)\n",
    "    ).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"This model has {} effective parameters.\".format(model.parameter_count(masked = True)))\n",
    "print(\"The conversion factor of this model is {} after layer {}.\".format(model.conversion_factor, model.conversion_layer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a dataset and define standard training and testing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                     transform=transforms.Compose([ \n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                            transforms.Resize((14,14))\n",
    "                        ]))\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, lengths=[int(0.9*len(dataset)), int(0.1*len(dataset))])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                            transforms.Resize((14,14))\n",
    "                        ])),\n",
    "    batch_size=128, shuffle=True)\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, epochs=10, val_loader=None, verbose=True):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0 and verbose:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "        if val_loader is not None:\n",
    "            print(\"Validation: \", end = \"\")\n",
    "            test(model, val_loader, criterion)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pretrain the model before changing its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, optimizer, criterion, epochs=5, val_loader=val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use NeurOps to optimize the model.\n",
    "\n",
    "First, we use a heuristic from `metrics.py` to measure the existing channels and neurons to determine which ones to prune. The simplest one is measuring the norm of incoming weights to a neuron. We'll copy the model (so we have access to the original), then score each neuron and prune the lowest scoring ones within each layer. After running the following block, try uncommenting different lines to see how different metrics affect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modded_model = copy.deepcopy(model)\n",
    "modded_optimizer = torch.optim.SGD(modded_model.parameters(), lr=0.01)\n",
    "modded_optimizer.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for i in range(len(model)-1):\n",
    "    scores = weight_sum(modded_model[i].weight)\n",
    "    # scores = weight_sum(modded_model[i].weight) +  weight_sum(modded_model[i+1].weight, fanin=False, conversion_factor=model.conversion_factor if i == model.conversion_layer else -1)\n",
    "    # scores = activation_variance(modded_model.activations[str(i)])\n",
    "    # scores = svd_score(modded_model.activations[str(i)])\n",
    "    # scores = nuclear_score(modded_model.activations[str(i)], average=i<3)\n",
    "    # scores = modded_model[i+1].batchnorm.weight.abs() if i != modded_model.conversion_layer else modded_model[i+1].batchnorm.weight.abs().reshape(modded_model.conversion_factor,-1).sum(0) \n",
    "    # Before trying this line, run the following block: # scores = fisher_info(mask_grads[i])\n",
    "    print(\"Layer {} scores: mean {:.3g}, std {:.3g}, min {:.3g}, smallest 25%:\".format(i, scores.mean(), scores.std(), scores.min()), end=\" \")\n",
    "    to_prune = np.argsort(scores.detach().cpu().numpy())[:int(0.25*len(scores))]\n",
    "    print(to_prune)\n",
    "    modded_model.prune(i, to_prune, optimizer=modded_optimizer, clear_activations=True)\n",
    "print(\"The pruned model has {} effective parameters.\".format(modded_model.parameter_count(masked = True)))\n",
    "print(\"Validation after pruning: \", end = \"\")\n",
    "test(modded_model, val_loader, criterion)\n",
    "train(modded_model, train_loader, modded_optimizer, criterion, epochs=2, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_mask_grads(model):\n",
    "    mask_grads = []\n",
    "    for i in range(len(model.activations)-1):\n",
    "        mask_grads.append(torch.empty(0, *model[i].mask_vector.shape))\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        for i in range(len(model)-1):\n",
    "            mask_grads[i] = torch.cat([mask_grads[i], model[i].mask_vector.grad.detach().cpu().unsqueeze(0)])\n",
    "    return mask_grads\n",
    "\n",
    "#mask_grads = collect_mask_grads(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try iterative pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modded_model_iterative = copy.deepcopy(model)\n",
    "modded_optimizer_iterative = torch.optim.SGD(modded_model_iterative.parameters(), lr=0.01)\n",
    "modded_optimizer_iterative.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for iter in range(5):\n",
    "    for i in range(len(modded_model_iterative)-1):\n",
    "        scores = weight_sum(modded_model_iterative[i].weight)\n",
    "        # scores = weight_sum(modded_model_iterative[i].weight) +  weight_sum(modded_model_iterative[i+1].weight, fanin=False, conversion_factor=modded_model_iterative.conversion_factor if i == modded_model_iterative.conversion_layer else -1)\n",
    "        # scores = activation_variance(modded_model_iterative.activations[str(i)])\n",
    "        # scores = svd_score(modded_model_iterative.activations[str(i)])\n",
    "        # scores = nuclear_score(modded_model_iterative.activations[str(i)], average=i<3)\n",
    "        # scores = modded_model_iterative[i+1].batchnorm.weight.abs() if i != modded_model_iterative.conversion_layer else modded_model_iterative[i+1].batchnorm.weight.abs().reshape(modded_model_iterative.conversion_factor,-1).sum(0) \n",
    "        print(\"Layer {} scores: mean {:.3g}, std {:.3g}, min {:.3g}, smallest 15%:\".format(i, scores.mean(), scores.std(), scores.min()), end=\" \")\n",
    "        to_prune = np.argsort(scores.cpu().detach().numpy())[:int(0.15*len(scores))]\n",
    "        print(to_prune)\n",
    "        modded_model_iterative.prune(i, to_prune, optimizer=modded_optimizer_iterative, clear_activations=True)\n",
    "    print(\"The pruned model now has {} effective parameters.\".format(modded_model_iterative.parameter_count(masked = True)))\n",
    "    print(\"Validation after pruning: \", end = \"\")\n",
    "    test(modded_model_iterative, val_loader, criterion)\n",
    "    train(modded_model_iterative, train_loader, modded_optimizer_iterative, criterion, epochs=2, val_loader=val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can also grow the model. The following cell uses a neurogenesis strategy similar to NORTH-Random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modded_model_grow = copy.deepcopy(model)\n",
    "modded_optimizer_grow = torch.optim.SGD(modded_model_grow.parameters(), lr=0.01)\n",
    "modded_optimizer_grow.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for iter in range(5):\n",
    "    for i in range(len(modded_model_grow)-1):\n",
    "        #score = orthogonality_gap(modded_model_grow.activations[str(i)])\n",
    "        max_rank = modded_model_grow[i].out_features if i > modded_model_grow.conversion_layer else modded_model_grow[i].out_channels\n",
    "        score = effective_rank(modded_model_grow.activations[str(i)])\n",
    "        to_add = max(score-int(0.95*max_rank), 0)\n",
    "        print(\"Layer {} score: {}/{}, neurons to add: {}\".format(i, score, max_rank, to_add))\n",
    "        modded_model_grow.grow(i, to_add, fanin_weights=\"iterative_orthogonalization\", \n",
    "                               optimizer=modded_optimizer_grow)\n",
    "    print(\"The grown model now has {} effective parameters.\".format(modded_model_grow.parameter_count(masked = True)))\n",
    "    print(\"Validation after growing: \", end = \"\")\n",
    "    test(modded_model_grow, val_loader, criterion)\n",
    "    train(modded_model_grow, train_loader, modded_optimizer_grow, criterion, epochs=2, val_loader=val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try masking neurons for simple grow-and-prune strategy, first doubling each layer's capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modded_model_masked = copy.deepcopy(model)\n",
    "modded_optimizer_masked = torch.optim.SGD(modded_model_masked.parameters(), lr=0.01)\n",
    "modded_optimizer_masked.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for i in range(len(modded_model_masked)-1):\n",
    "    neurons = modded_model_masked[i].out_features if i > modded_model_masked.conversion_layer else modded_model_masked[i].out_channels\n",
    "    modded_model_masked.grow(i, neurons, fanin_weights=\"kaiming\", fanout_weights=\"kaiming\", optimizer=modded_optimizer_masked)\n",
    "    modded_model_masked.mask(i, list(range(neurons, 2*neurons)))\n",
    "\n",
    "for iter in range(5):\n",
    "    for i in range(len(modded_model_masked)-1):\n",
    "        scores = weight_sum(modded_model_masked[i].get_weights())\n",
    "        print(\"Layer {} scores: mean {:.3g}, std {:.3g}, min {:.3g}, smallest 25% to mask:\".format(i, scores[scores != 0].mean(), scores[scores != 0].std(), scores[scores != 0].min()), end=\" \")\n",
    "        to_mask = np.argsort(scores.detach().numpy())[sum(scores == 0):sum(scores == 0)+int(0.25*sum(scores != 0))]\n",
    "        print(to_mask, end=\", \")\n",
    "        modded_model_masked.mask(i, to_mask)\n",
    "        to_unmask = np.argsort(scores.detach().numpy())[:sum(scores == 0)]\n",
    "        to_unmask = np.random.choice(to_unmask, size=len(to_mask), replace=False)\n",
    "        print(\"random neurons to unmask:\", to_unmask)\n",
    "        modded_model_masked.unmask(i, to_unmask, optimizer=modded_optimizer_masked)\n",
    "    print(\"The masked model now has {} effective parameters.\".format(modded_model_masked.parameter_count(masked = True)))\n",
    "    print(\"Validation after growing: \", end = \"\")\n",
    "    test(modded_model_masked, val_loader, criterion)\n",
    "    train(modded_model_masked, train_loader, modded_optimizer_masked, criterion, epochs=2, val_loader=val_loader, verbose=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17b88845c8368f4e9bf0ab0c6bd871dae3b65fc6e015c8990d2b5b0cf4897a6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
