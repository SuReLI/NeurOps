{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial walks through some uses of the PyTorch implementation of NeurOps.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the PyTorch implementation and some other useful packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch.neurops import *\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a LeNet-style model, which has three convolutional model followed by two fully-connected model. We use the `ModSequential` class to wrap the `ModConv2d` and `ModLinear` model, which allows us to mask, prune, and grow the model. We also use the `track_activations` and `track_auxiliary_gradients` arguments to enable the tracking of activations and auxiliary gradients later. By adding the `input_shape` of the data, we can compute the conversion factor of how many input neurons to add to the first linear layer when a new output channel is added to the final convolutional layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 15634 effective parameters.\n",
      "The conversion factor of this model is 4 after layer 2.\n"
     ]
    }
   ],
   "source": [
    "model = ModSequential(\n",
    "        ModConv2d(in_channels=1, out_channels=8, kernel_size=7, masked=True, padding=1, learnable_mask=True),\n",
    "        ModConv2d(in_channels=8, out_channels=16, kernel_size=7, masked=True, padding=1, prebatchnorm=True, learnable_mask=True),\n",
    "        ModConv2d(in_channels=16, out_channels=16, kernel_size=5, masked=True, prebatchnorm=True, learnable_mask=True),\n",
    "        ModLinear(64, 32, masked=True, prebatchnorm=True, learnable_mask=True),\n",
    "        ModLinear(32, 10, masked=True, prebatchnorm=True),\n",
    "        track_activations=True,\n",
    "        track_auxiliary_gradients=True,\n",
    "        input_shape = (1, 14, 14)\n",
    "    ).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"This model has {} effective parameters.\".format(model.parameter_count(masked = True)))\n",
    "print(\"The conversion factor of this model is {} after layer {}.\".format(model.conversion_factor, model.conversion_layer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a dataset and define standard training and testing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                     transform=transforms.Compose([ \n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                            transforms.Resize((14,14))\n",
    "                        ]))\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, lengths=[int(0.9*len(dataset)), int(0.1*len(dataset))])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                            transforms.Resize((14,14))\n",
    "                        ])),\n",
    "    batch_size=128, shuffle=True)\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, epochs=10, val_loader=None, verbose=True):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0 and verbose:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "        if val_loader is not None:\n",
    "            print(\"Validation: \", end = \"\")\n",
    "            test(model, val_loader, criterion)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pretrain the model before changing its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 2.411193\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.864118\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.518800\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.358262\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.311406\n",
      "Validation: Average loss: 0.0024, Accuracy: 5666/6000 (94.43%)\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, optimizer, criterion, epochs=5, val_loader=val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use NeurOps to optimize the model.\n",
    "\n",
    "First, we use a heuristic from `metrics.py` to measure the existing channels and neurons to determine which ones to prune. The simplest one is measuring the norm of incoming weights to a neuron. We'll copy the model (so we have access to the original), then score each neuron and prune the lowest scoring ones within each layer. After running the following block, try uncommenting different lines to see how different metrics affect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 scores: mean 3.48, std 0.163, min 3.3, smallest 25%: [1 3]\n",
      "Layer 1 scores: mean 7.32, std 0.239, min 6.74, smallest 25%: [4 8 7 2]\n",
      "Layer 2 scores: mean 7.38, std 0.219, min 7.03, smallest 25%: [ 4 10  2 14]\n",
      "Layer 3 scores: mean 2.97, std 0.271, min 2.48, smallest 25%: [19 26  8 30  7  0 25  1]\n",
      "The pruned model has 9058 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0111, Accuracy: 3967/6000 (66.12%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.949849\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.415809\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.273189\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.297271\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.202424\n",
      "Validation: Average loss: 0.0017, Accuracy: 5742/6000 (95.70%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.144360\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.258612\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.113762\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.174505\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.200316\n",
      "Validation: Average loss: 0.0010, Accuracy: 5788/6000 (96.47%)\n"
     ]
    }
   ],
   "source": [
    "modded_model = copy.deepcopy(model)\n",
    "modded_optimizer = torch.optim.SGD(modded_model.parameters(), lr=0.01)\n",
    "modded_optimizer.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for i in range(len(model)-1):\n",
    "    scores = weight_sum(modded_model[i].weight)\n",
    "    # scores = weight_sum(modded_model[i].weight) +  weight_sum(modded_model[i+1].weight, fanin=False, conversion_factor=model.conversion_factor if i == model.conversion_layer else -1)\n",
    "    # scores = activation_variance(modded_model.activations[str(i)])\n",
    "    # scores = svd_score(modded_model.activations[str(i)])\n",
    "    # scores = nuclear_score(modded_model.activations[str(i)], average=i<3)\n",
    "    # scores = modded_model[i+1].batchnorm.weight.abs() if i != modded_model.conversion_layer else modded_model[i+1].batchnorm.weight.abs().reshape(modded_model.conversion_factor,-1).sum(0) \n",
    "    # Before trying this line, run the following block: # scores = fisher_info(mask_grads[i])\n",
    "    print(\"Layer {} scores: mean {:.3g}, std {:.3g}, min {:.3g}, smallest 25%:\".format(i, scores.mean(), scores.std(), scores.min()), end=\" \")\n",
    "    to_prune = np.argsort(scores.detach().cpu().numpy())[:int(0.25*len(scores))]\n",
    "    print(to_prune)\n",
    "    modded_model.prune(i, to_prune, optimizer=modded_optimizer, clear_activations=True)\n",
    "print(\"The pruned model has {} effective parameters.\".format(modded_model.parameter_count(masked = True)))\n",
    "print(\"Validation after pruning: \", end = \"\")\n",
    "test(modded_model, val_loader, criterion)\n",
    "train(modded_model, train_loader, modded_optimizer, criterion, epochs=2, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_mask_grads(model):\n",
    "    mask_grads = []\n",
    "    for i in range(len(model.activations)-1):\n",
    "        mask_grads.append(torch.empty(0, *model[i].mask_vector.shape))\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        for i in range(len(model)-1):\n",
    "            mask_grads[i] = torch.cat([mask_grads[i], model[i].mask_vector.grad.detach().cpu().unsqueeze(0)])\n",
    "    return mask_grads\n",
    "\n",
    "#mask_grads = collect_mask_grads(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try iterative pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 scores: mean 3.48, std 0.163, min 3.3, smallest 15%: [1]\n",
      "Layer 1 scores: mean 8.55, std 0.235, min 8.21, smallest 15%: [4 8]\n",
      "Layer 2 scores: mean 8.62, std 0.242, min 8.11, smallest 15%: [10  2]\n",
      "Layer 3 scores: mean 3.44, std 0.285, min 2.81, smallest 15%: [26 19  8  1]\n",
      "The pruned model now has 12176 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0058, Accuracy: 5425/6000 (90.42%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.545421\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.318735\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.195651\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.215880\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.136218\n",
      "Validation: Average loss: 0.0015, Accuracy: 5753/6000 (95.88%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.178195\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.119451\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.076241\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.110265\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.063840\n",
      "Validation: Average loss: 0.0009, Accuracy: 5808/6000 (96.80%)\n",
      "Layer 0 scores: mean 3.58, std 0.136, min 3.43, smallest 15%: [3]\n",
      "Layer 1 scores: mean 7.64, std 0.249, min 7.17, smallest 15%: [ 4 11]\n",
      "Layer 2 scores: mean 7.64, std 0.214, min 7.29, smallest 15%: [3 7]\n",
      "Layer 3 scores: mean 3.07, std 0.231, min 2.56, smallest 15%: [ 6 24 26  0]\n",
      "The pruned model now has 9058 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0036, Accuracy: 5227/6000 (87.12%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.438986\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.241555\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.183303\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.168454\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.134504\n",
      "Validation: Average loss: 0.0012, Accuracy: 5821/6000 (97.02%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.236878\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.177985\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.100159\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.041368\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.094188\n",
      "Validation: Average loss: 0.0008, Accuracy: 5822/6000 (97.03%)\n",
      "Layer 0 scores: mean 3.69, std 0.139, min 3.49, smallest 15%: []\n",
      "Layer 1 scores: mean 7.96, std 0.22, min 7.65, smallest 15%: [11]\n",
      "Layer 2 scores: mean 7.35, std 0.308, min 6.85, smallest 15%: [6]\n",
      "Layer 3 scores: mean 2.93, std 0.184, min 2.57, smallest 15%: [16  9  8]\n",
      "The pruned model now has 7910 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0013, Accuracy: 5735/6000 (95.58%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.239042\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.192573\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.118042\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.106548\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.122079\n",
      "Validation: Average loss: 0.0012, Accuracy: 5787/6000 (96.45%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.160990\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.046387\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.077848\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.113712\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.102610\n",
      "Validation: Average loss: 0.0007, Accuracy: 5836/6000 (97.27%)\n",
      "Layer 0 scores: mean 3.8, std 0.137, min 3.59, smallest 15%: []\n",
      "Layer 1 scores: mean 8.29, std 0.237, min 8, smallest 15%: [6]\n",
      "Layer 2 scores: mean 7.03, std 0.318, min 6.66, smallest 15%: [6]\n",
      "Layer 3 scores: mean 2.75, std 0.17, min 2.39, smallest 15%: [20 16  2]\n",
      "The pruned model now has 6836 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0021, Accuracy: 5592/6000 (93.20%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.245072\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.175443\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.166997\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.137441\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.093632\n",
      "Validation: Average loss: 0.0010, Accuracy: 5842/6000 (97.37%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.087045\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.102226\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.111747\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.119139\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.102254\n",
      "Validation: Average loss: 0.0007, Accuracy: 5857/6000 (97.62%)\n",
      "Layer 0 scores: mean 3.88, std 0.115, min 3.71, smallest 15%: []\n",
      "Layer 1 scores: mean 8.64, std 0.247, min 8.35, smallest 15%: [6]\n",
      "Layer 2 scores: mean 6.64, std 0.289, min 6.1, smallest 15%: [0]\n",
      "Layer 3 scores: mean 2.6, std 0.186, min 2.26, smallest 15%: [12 15]\n",
      "The pruned model now has 5885 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0019, Accuracy: 5588/6000 (93.13%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.298688\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.147758\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.096114\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.120361\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.092680\n",
      "Validation: Average loss: 0.0009, Accuracy: 5849/6000 (97.48%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.055574\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.058239\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.072855\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.088622\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.034792\n",
      "Validation: Average loss: 0.0008, Accuracy: 5834/6000 (97.23%)\n"
     ]
    }
   ],
   "source": [
    "modded_model_iterative = copy.deepcopy(model)\n",
    "modded_optimizer_iterative = torch.optim.SGD(modded_model_iterative.parameters(), lr=0.01)\n",
    "modded_optimizer_iterative.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for iter in range(5):\n",
    "    for i in range(len(modded_model_iterative)-1):\n",
    "        scores = weight_sum(modded_model_iterative[i].weight)\n",
    "        # scores = weight_sum(model[i].weight) +  weight_sum(model[i+1].weight, fanin=False, conversion_factor=model.conversion_factor if i == model.conversion_layer else -1)\n",
    "        # scores = activation_variance(model.activations[str(i)])\n",
    "        # scores = svd_score(model.activations[str(i)])\n",
    "        # scores = nuclear_score(model.activations[str(i)], average=i<3)\n",
    "        # scores = model[i+1].batchnorm.weight.abs() if i != model.conversion_layer else model[i+1].batchnorm.weight.abs().reshape(model.conversion_factor,-1).sum(0) \n",
    "        print(\"Layer {} scores: mean {:.3g}, std {:.3g}, min {:.3g}, smallest 15%:\".format(i, scores.mean(), scores.std(), scores.min()), end=\" \")\n",
    "        to_prune = np.argsort(scores.cpu().detach().numpy())[:int(0.15*len(scores))]\n",
    "        print(to_prune)\n",
    "        modded_model_iterative.prune(i, to_prune, optimizer=modded_optimizer_iterative, clear_activations=True)\n",
    "    print(\"The pruned model now has {} effective parameters.\".format(modded_model_iterative.parameter_count(masked = True)))\n",
    "    print(\"Validation after pruning: \", end = \"\")\n",
    "    test(modded_model_iterative, val_loader, criterion)\n",
    "    train(modded_model_iterative, train_loader, modded_optimizer_iterative, criterion, epochs=2, val_loader=val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can also grow the model. The following cell uses a neurogenesis strategy similar to NORTH-Random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 score: 8/8, neurons to add: 1\n",
      "torch.Size([9, 1, 7, 7])\n",
      "torch.Size([9, 49])\n",
      "Layer 1 score: 16/16, neurons to add: 1\n",
      "torch.Size([17, 9, 7, 7])\n",
      "torch.Size([17, 441])\n",
      "Layer 2 score: 16/16, neurons to add: 1\n",
      "torch.Size([17, 17, 5, 5])\n",
      "torch.Size([17, 425])\n",
      "Layer 3 score: 32/32, neurons to add: 2\n",
      "torch.Size([34, 68])\n",
      "torch.Size([32, 68])\n",
      "The grown model now has 16731 effective parameters.\n",
      "Validation after growing: Average loss: 0.0024, Accuracy: 5666/6000 (94.43%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.311449\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.275011\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.222741\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.158694\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.134241\n",
      "Validation: Average loss: 0.0013, Accuracy: 5772/6000 (96.20%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.153834\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.112107\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.052323\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.127757\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.034666\n",
      "Validation: Average loss: 0.0009, Accuracy: 5806/6000 (96.77%)\n",
      "Layer 0 score: 9/9, neurons to add: 1\n",
      "torch.Size([10, 1, 7, 7])\n",
      "torch.Size([10, 49])\n",
      "Layer 1 score: 17/17, neurons to add: 1\n",
      "torch.Size([18, 10, 7, 7])\n",
      "torch.Size([18, 490])\n",
      "Layer 2 score: 17/17, neurons to add: 1\n",
      "torch.Size([18, 18, 5, 5])\n",
      "torch.Size([18, 450])\n",
      "Layer 3 score: 34/34, neurons to add: 2\n",
      "torch.Size([36, 72])\n",
      "torch.Size([34, 72])\n",
      "The grown model now has 19217 effective parameters.\n",
      "Validation after growing: Average loss: 0.0009, Accuracy: 5806/6000 (96.77%)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (34) must match the size of tensor b (36) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mValidation after growing: \u001b[39m\u001b[39m\"\u001b[39m, end \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m test(modded_model_grow, val_loader, criterion)\n\u001b[0;32m---> 17\u001b[0m train(modded_model_grow, train_loader, modded_optimizer_grow, criterion, epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, val_loader\u001b[39m=\u001b[39;49mval_loader)\n",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion, epochs, val_loader, verbose)\u001b[0m\n\u001b[1;32m     25\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     26\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[0;32m---> 27\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     28\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/repos/NeurOps/pytorch/venv/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/repos/NeurOps/pytorch/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (34) must match the size of tensor b (36) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "modded_model_grow = copy.deepcopy(model)\n",
    "modded_optimizer_grow = torch.optim.SGD(modded_model_grow.parameters(), lr=0.01)\n",
    "modded_optimizer_grow.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for iter in range(5):\n",
    "    for i in range(len(modded_model_grow)-1):\n",
    "        #score = orthogonality_gap(modded_model_grow.activations[str(i)])\n",
    "        max_rank = modded_model_grow[i].out_features if i > modded_model_grow.conversion_layer else modded_model_grow[i].out_channels\n",
    "        score = effective_rank(modded_model_grow.activations[str(i)])\n",
    "        to_add = max(score-int(0.95*max_rank), 0)\n",
    "        print(\"Layer {} score: {}/{}, neurons to add: {}\".format(i, score, max_rank, to_add))\n",
    "        modded_model_grow.grow(i, to_add, fanin_weights=\"iterative_orthogonalization\", \n",
    "                               optimizer=modded_optimizer_grow)\n",
    "    print(\"The grown model now has {} effective parameters.\".format(modded_model_grow.parameter_count(masked = True)))\n",
    "    print(\"Validation after growing: \", end = \"\")\n",
    "    test(modded_model_grow, val_loader, criterion)\n",
    "    train(modded_model_grow, train_loader, modded_optimizer_grow, criterion, epochs=2, val_loader=val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try masking neurons for simple grow-and-prune strategy, first doubling each layer's capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modded_model_masked = copy.deepcopy(model)\n",
    "modded_optimizer_masked = torch.optim.SGD(modded_model_masked.parameters(), lr=0.01)\n",
    "modded_optimizer_masked.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for i in range(len(modded_model_masked)-1):\n",
    "    neurons = modded_model_masked[i].out_features if i > modded_model_masked.conversion_layer else modded_model_masked[i].out_channels\n",
    "    modded_model_masked.grow(i, neurons, fanin_weights=\"kaiming\", fanout_weights=\"kaiming\", optimizer=modded_optimizer_masked)\n",
    "    modded_model_masked.mask(i, list(range(neurons, 2*neurons)))\n",
    "\n",
    "for iter in range(5):\n",
    "    for i in range(len(modded_model_masked)-1):\n",
    "        scores = weight_sum(modded_model_masked[i].get_weights())\n",
    "        print(\"Layer {} scores: mean {:.3g}, std {:.3g}, min {:.3g}, smallest 25% to mask:\".format(i, scores[scores != 0].mean(), scores[scores != 0].std(), scores[scores != 0].min()), end=\" \")\n",
    "        to_mask = np.argsort(scores.detach().numpy())[sum(scores == 0):sum(scores == 0)+int(0.25*sum(scores != 0))]\n",
    "        print(to_mask, end=\", \")\n",
    "        modded_model_masked.mask(i, to_mask)\n",
    "        to_unmask = np.argsort(scores.detach().numpy())[:sum(scores == 0)]\n",
    "        to_unmask = np.random.choice(to_unmask, size=len(to_mask), replace=False)\n",
    "        print(\"random neurons to unmask:\", to_unmask)\n",
    "        modded_model_masked.unmask(i, to_unmask, optimizer=modded_optimizer_masked)\n",
    "    print(\"The masked model now has {} effective parameters.\".format(modded_model_masked.parameter_count(masked = True)))\n",
    "    print(\"Validation after growing: \", end = \"\")\n",
    "    test(modded_model_masked, val_loader, criterion)\n",
    "    train(modded_model_masked, train_loader, modded_optimizer_masked, criterion, epochs=2, val_loader=val_loader, verbose=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17b88845c8368f4e9bf0ab0c6bd871dae3b65fc6e015c8990d2b5b0cf4897a6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
