{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial walks through some uses of the PyTorch implementation of NeurOps.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the PyTorch implementation and some other useful packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaitlin/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from pytorch.src import *\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a LeNet-style model, which has three convolutional model followed by two fully-connected model. We use the `ModSequential` class to wrap the `ModConv2d` and `ModLinear` model, which allows us to mask, prune, and grow the model. We also use the `track_activations` and `track_auxiliary_gradients` arguments to enable the tracking of activations and auxiliary gradients later. By adding the `input_shape` of the data, we can compute the conversion factor of how many input neurons to add to the first linear layer when a new output channel is added to the final convolutional layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 15538 effective parameters.\n",
      "The conversion factor of this model is 4 after layer 2.\n"
     ]
    }
   ],
   "source": [
    "model = ModSequential(\n",
    "        ModConv2d(in_channels=1, out_channels=8, kernel_size=7, masked=True, padding=1, learnable_mask=True),\n",
    "        ModConv2d(in_channels=8, out_channels=16, kernel_size=7, masked=True, padding=1, prebatchnorm=True, learnable_mask=True),\n",
    "        ModConv2d(in_channels=16, out_channels=16, kernel_size=5, masked=True, prebatchnorm=True, learnable_mask=True),\n",
    "        ModLinear(64, 32, masked=True, learnable_mask=True),\n",
    "        ModLinear(32, 10, masked=True),\n",
    "        track_activations=True,\n",
    "        track_auxiliary_gradients=True,\n",
    "        input_shape = (1, 14, 14)\n",
    "    ).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"This model has {} effective parameters.\".format(model.parameter_count(masked = True)))\n",
    "print(\"The conversion factor of this model is {} after layer {}.\".format(model.conversion_factor, model.conversion_layer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a dataset and define standard training and testing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                     transform=transforms.Compose([ \n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                            transforms.Resize((14,14))\n",
    "                        ]))\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, lengths=[int(0.9*len(dataset)), int(0.1*len(dataset))])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                            transforms.Resize((14,14))\n",
    "                        ])),\n",
    "    batch_size=128, shuffle=True)\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, epochs=10, val_loader=None, verbose=True):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0 and verbose:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "        if val_loader is not None:\n",
    "            print(\"Validation: \", end = \"\")\n",
    "            test(model, val_loader, criterion)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pretrain the model before changing its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 2.300797\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 2.128591\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 1.599569\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.988867\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.706234\n",
      "Validation: Average loss: 0.0059, Accuracy: 4720/6000 (78.67%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.734146\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.645335\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.564983\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.299784\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.328388\n",
      "Validation: Average loss: 0.0018, Accuracy: 5607/6000 (93.45%)\n",
      "Train Epoch: 2 [0/54000 (0%)]\tLoss: 0.200425\n",
      "Train Epoch: 2 [12800/54000 (24%)]\tLoss: 0.139246\n",
      "Train Epoch: 2 [25600/54000 (47%)]\tLoss: 0.245554\n",
      "Train Epoch: 2 [38400/54000 (71%)]\tLoss: 0.195658\n",
      "Train Epoch: 2 [51200/54000 (95%)]\tLoss: 0.220021\n",
      "Validation: Average loss: 0.0013, Accuracy: 5702/6000 (95.03%)\n",
      "Train Epoch: 3 [0/54000 (0%)]\tLoss: 0.085208\n",
      "Train Epoch: 3 [12800/54000 (24%)]\tLoss: 0.075280\n",
      "Train Epoch: 3 [25600/54000 (47%)]\tLoss: 0.114262\n",
      "Train Epoch: 3 [38400/54000 (71%)]\tLoss: 0.138246\n",
      "Train Epoch: 3 [51200/54000 (95%)]\tLoss: 0.084886\n",
      "Validation: Average loss: 0.0012, Accuracy: 5729/6000 (95.48%)\n",
      "Train Epoch: 4 [0/54000 (0%)]\tLoss: 0.154037\n",
      "Train Epoch: 4 [12800/54000 (24%)]\tLoss: 0.113768\n",
      "Train Epoch: 4 [25600/54000 (47%)]\tLoss: 0.106094\n",
      "Train Epoch: 4 [38400/54000 (71%)]\tLoss: 0.075518\n",
      "Train Epoch: 4 [51200/54000 (95%)]\tLoss: 0.057111\n",
      "Validation: Average loss: 0.0009, Accuracy: 5779/6000 (96.32%)\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, optimizer, criterion, epochs=5, val_loader=val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use NeurOps to optimize the model.\n",
    "\n",
    "First, we use a heuristic from `metrics.py` to measure the existing channels and neurons to determine which ones to prune. The simplest one is measuring the norm of incoming weights to a neuron. We'll copy the model (so we have access to the original), then score each neuron and prune the lowest scoring ones within each layer. After running the following block, try uncommenting different lines to see how different metrics affect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 scores: mean 3.96, std 0.184, min 3.74, smallest 25%: [7 0]\n",
      "Layer 1 scores: mean 11, std 0.502, min 9.76, smallest 25%: [ 2 12  8  5]\n",
      "Layer 2 scores: mean 12.1, std 1.23, min 10.1, smallest 25%: [ 9  1 12 13]\n",
      "Layer 3 scores: mean 4.51, std 0.498, min 3.74, smallest 25%: [ 7  1  5 10  4 16 15 24]\n",
      "The pruned model has 8914 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0063, Accuracy: 4348/6000 (72.47%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 1.161084\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.276130\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.195021\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.150776\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.110889\n",
      "Validation: Average loss: 0.0011, Accuracy: 5763/6000 (96.05%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.145399\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.137830\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.135050\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.117636\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.030834\n",
      "Validation: Average loss: 0.0008, Accuracy: 5789/6000 (96.48%)\n"
     ]
    }
   ],
   "source": [
    "modded_model = copy.deepcopy(model)\n",
    "modded_optimizer = torch.optim.SGD(modded_model.parameters(), lr=0.01)\n",
    "modded_optimizer.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for i in range(len(model)-1):\n",
    "    scores = weight_sum(model[i].weight)\n",
    "    # scores = weight_sum(model[i].weight) +  weight_sum(model[i+1].weight, fanin=False, conversion_factor=model.conversion_factor if i == model.conversion_layer else -1)\n",
    "    # scores = activation_variance(model.activations[str(i)])\n",
    "    # scores = svd_score(model.activations[str(i)])\n",
    "    # scores = nuclear_score(model.activations[str(i)], average=i<3)\n",
    "    # Before trying this line, run the following block: # scores = fisher_info(mask_grads[i])\n",
    "    print(\"Layer {} scores: mean {:.3g}, std {:.3g}, min {:.3g}, smallest 25%:\".format(i, scores.mean(), scores.std(), scores.min()), end=\" \")\n",
    "    to_prune = np.argsort(scores.detach().numpy())[:int(0.25*len(scores))]\n",
    "    print(to_prune)\n",
    "    modded_model.prune(i, to_prune, optimizer=modded_optimizer, clear_activations=True)\n",
    "print(\"The pruned model has {} effective parameters.\".format(modded_model.parameter_count(masked = True)))\n",
    "print(\"Validation after pruning: \", end = \"\")\n",
    "test(modded_model, val_loader, criterion)\n",
    "train(modded_model, train_loader, modded_optimizer, criterion, epochs=2, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_mask_grads():\n",
    "    mask_grads = []\n",
    "    for i in range(len(model.activations)-1):\n",
    "        mask_grads.append(torch.empty(0, *model[i].mask_vector.shape))\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        for i in range(len(model.activations)-1):\n",
    "            mask_grads[i] = torch.cat([mask_grads[i], model[i].mask_vector.grad.unsqueeze(0)])\n",
    "    return mask_grads\n",
    "\n",
    "#mask_grads = collect_mask_grads()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try iterative pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 scores: mean 3.96, std 0.184, min 3.74, smallest 15%: [7]\n",
      "Layer 1 scores: mean 9.63, std 0.421, min 8.46, smallest 15%: [ 2 12]\n",
      "Layer 2 scores: mean 10.6, std 1.07, min 8.81, smallest 15%: [ 9 13]\n",
      "Layer 3 scores: mean 4, std 0.494, min 3.14, smallest 15%: [ 7  5 10  1]\n",
      "The pruned model now has 12008 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0032, Accuracy: 5220/6000 (87.00%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 1.020988\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.232090\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.188705\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.060886\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.171708\n",
      "Validation: Average loss: 0.0009, Accuracy: 5792/6000 (96.53%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.097389\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.070464\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.054296\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.092534\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.083284\n",
      "Validation: Average loss: 0.0008, Accuracy: 5808/6000 (96.80%)\n",
      "Layer 0 scores: mean 4.09, std 0.208, min 3.81, smallest 15%: [0]\n",
      "Layer 1 scores: mean 8.75, std 0.275, min 8.29, smallest 15%: [13  4]\n",
      "Layer 2 scores: mean 10.5, std 1.18, min 8.64, smallest 15%: [1 0]\n",
      "Layer 3 scores: mean 3.84, std 0.476, min 2.95, smallest 15%: [ 3  2 11 20]\n",
      "The pruned model now has 8914 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0019, Accuracy: 5593/6000 (93.22%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.213269\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.125065\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.110761\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.120215\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.117750\n",
      "Validation: Average loss: 0.0008, Accuracy: 5827/6000 (97.12%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.109505\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.086576\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.121977\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.050287\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.070326\n",
      "Validation: Average loss: 0.0007, Accuracy: 5828/6000 (97.13%)\n",
      "Layer 0 scores: mean 4.22, std 0.205, min 4.03, smallest 15%: []\n",
      "Layer 1 scores: mean 9.1, std 0.258, min 8.82, smallest 15%: [7]\n",
      "Layer 2 scores: mean 10.5, std 1.14, min 9.08, smallest 15%: [9]\n",
      "Layer 3 scores: mean 3.83, std 0.47, min 3.01, smallest 15%: [ 7 17 21]\n",
      "The pruned model now has 7780 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0017, Accuracy: 5548/6000 (92.47%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.185465\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.083087\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.082873\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.085358\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.092936\n",
      "Validation: Average loss: 0.0008, Accuracy: 5820/6000 (97.00%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.146434\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.079957\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.119592\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.056444\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.112606\n",
      "Validation: Average loss: 0.0007, Accuracy: 5821/6000 (97.02%)\n",
      "Layer 0 scores: mean 4.31, std 0.22, min 4.11, smallest 15%: []\n",
      "Layer 1 scores: mean 9.41, std 0.303, min 9.01, smallest 15%: [7]\n",
      "Layer 2 scores: mean 10.3, std 1.12, min 8.88, smallest 15%: [8]\n",
      "Layer 3 scores: mean 3.73, std 0.388, min 3.21, smallest 15%: [15 19  8]\n",
      "The pruned model now has 6720 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0019, Accuracy: 5512/6000 (91.87%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.188354\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.105954\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.099045\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.143025\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.057796\n",
      "Validation: Average loss: 0.0007, Accuracy: 5829/6000 (97.15%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.038166\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.050727\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.071069\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.078458\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.072855\n",
      "Validation: Average loss: 0.0007, Accuracy: 5839/6000 (97.32%)\n",
      "Layer 0 scores: mean 4.4, std 0.232, min 4.13, smallest 15%: []\n",
      "Layer 1 scores: mean 9.76, std 0.301, min 9.41, smallest 15%: [4]\n",
      "Layer 2 scores: mean 9.96, std 0.965, min 8.44, smallest 15%: [4]\n",
      "Layer 3 scores: mean 3.61, std 0.384, min 2.85, smallest 15%: [15  5]\n",
      "The pruned model now has 5781 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0012, Accuracy: 5718/6000 (95.30%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.158982\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.143886\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.051963\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.059393\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.087208\n",
      "Validation: Average loss: 0.0007, Accuracy: 5821/6000 (97.02%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.116309\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.106080\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.110765\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.075401\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.115001\n",
      "Validation: Average loss: 0.0007, Accuracy: 5816/6000 (96.93%)\n"
     ]
    }
   ],
   "source": [
    "modded_model_iterative = copy.deepcopy(model)\n",
    "modded_optimizer_iterative = torch.optim.SGD(modded_model_iterative.parameters(), lr=0.01)\n",
    "modded_optimizer_iterative.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for iter in range(5):\n",
    "    for i in range(len(modded_model_iterative)-1):\n",
    "        scores = weight_sum(modded_model_iterative[i].weight)\n",
    "        # scores = weight_sum(model[i].weight) +  weight_sum(model[i+1].weight, fanin=False, conversion_factor=model.conversion_factor if i == model.conversion_layer else -1)\n",
    "        # scores = activation_variance(model.activations[str(i)])\n",
    "        # scores = svd_score(model.activations[str(i)])\n",
    "        # scores = nuclear_score(model.activations[str(i)], average=i<3)\n",
    "        print(\"Layer {} scores: mean {:.3g}, std {:.3g}, min {:.3g}, smallest 15%:\".format(i, scores.mean(), scores.std(), scores.min()), end=\" \")\n",
    "        to_prune = np.argsort(scores.detach().numpy())[:int(0.15*len(scores))]\n",
    "        print(to_prune)\n",
    "        modded_model_iterative.prune(i, to_prune, optimizer=modded_optimizer_iterative, clear_activations=True)\n",
    "    print(\"The pruned model now has {} effective parameters.\".format(modded_model_iterative.parameter_count(masked = True)))\n",
    "    print(\"Validation after pruning: \", end = \"\")\n",
    "    test(modded_model_iterative, val_loader, criterion)\n",
    "    train(modded_model_iterative, train_loader, modded_optimizer_iterative, criterion, epochs=2, val_loader=val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can also grow the model. The following cell uses a neurogenesis strategy similar to NORTH-Random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 score: 8/8, neurons to add: 1\n",
      "Layer 1 score: 16/16, neurons to add: 1\n",
      "Layer 2 score: 16/16, neurons to add: 1\n",
      "Layer 3 score: 30/32, neurons to add: 0\n",
      "The grown model now has 16405 effective parameters.\n",
      "Validation after growing: Average loss: 0.0007, Accuracy: 5826/6000 (97.10%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.605522\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.118547\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.143367\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.084291\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.103274\n",
      "Validation: Average loss: 0.0008, Accuracy: 5811/6000 (96.85%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.040805\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.100506\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.040410\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.038195\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.042951\n",
      "Validation: Average loss: 0.0007, Accuracy: 5828/6000 (97.13%)\n",
      "Layer 0 score: 9/9, neurons to add: 1\n",
      "Layer 1 score: 17/17, neurons to add: 1\n",
      "Layer 2 score: 17/17, neurons to add: 1\n",
      "Layer 3 score: 30/32, neurons to add: 0\n",
      "The grown model now has 18713 effective parameters.\n",
      "Validation after growing: Average loss: 0.0007, Accuracy: 5828/6000 (97.13%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.083486\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.107323\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.035888\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.122458\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.050644\n",
      "Validation: Average loss: 0.0007, Accuracy: 5834/6000 (97.23%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.080988\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.058502\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.032951\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.059402\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.105877\n",
      "Validation: Average loss: 0.0006, Accuracy: 5833/6000 (97.22%)\n",
      "Layer 0 score: 10/10, neurons to add: 1\n",
      "Layer 1 score: 18/18, neurons to add: 1\n",
      "Layer 2 score: 18/18, neurons to add: 1\n",
      "Layer 3 score: 30/32, neurons to add: 0\n",
      "The grown model now has 21168 effective parameters.\n",
      "Validation after growing: Average loss: 0.0006, Accuracy: 5833/6000 (97.22%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.078407\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.040627\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.086643\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.044618\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.026505\n",
      "Validation: Average loss: 0.0005, Accuracy: 5876/6000 (97.93%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.052506\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.025954\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.052274\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.016827\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.072332\n",
      "Validation: Average loss: 0.0005, Accuracy: 5877/6000 (97.95%)\n",
      "Layer 0 score: 11/11, neurons to add: 1\n",
      "Layer 1 score: 19/19, neurons to add: 1\n",
      "Layer 2 score: 19/19, neurons to add: 1\n",
      "Layer 3 score: 30/32, neurons to add: 0\n",
      "The grown model now has 23772 effective parameters.\n",
      "Validation after growing: Average loss: 0.0005, Accuracy: 5877/6000 (97.95%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.078658\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.051826\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.064973\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.048153\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.104501\n",
      "Validation: Average loss: 0.0005, Accuracy: 5869/6000 (97.82%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.027167\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.040287\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.033001\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.068071\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.054151\n",
      "Validation: Average loss: 0.0004, Accuracy: 5886/6000 (98.10%)\n",
      "Layer 0 score: 12/12, neurons to add: 1\n",
      "Layer 1 score: 20/20, neurons to add: 1\n",
      "Layer 2 score: 20/20, neurons to add: 1\n",
      "Layer 3 score: 30/32, neurons to add: 0\n",
      "The grown model now has 26525 effective parameters.\n",
      "Validation after growing: Average loss: 0.0004, Accuracy: 5886/6000 (98.10%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.025211\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.094490\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.047705\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.094036\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.039319\n",
      "Validation: Average loss: 0.0004, Accuracy: 5894/6000 (98.23%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.035509\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.043737\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.038620\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.023285\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.028201\n",
      "Validation: Average loss: 0.0004, Accuracy: 5894/6000 (98.23%)\n"
     ]
    }
   ],
   "source": [
    "modded_model_grow = copy.deepcopy(model)\n",
    "modded_optimizer_grow = torch.optim.SGD(modded_model_grow.parameters(), lr=0.01)\n",
    "modded_optimizer_grow.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for iter in range(5):\n",
    "    for i in range(len(modded_model_grow)-1):\n",
    "        #score = orthogonality_gap(modded_model_grow.activations[str(i)])\n",
    "        max_rank = modded_model_grow[i].out_features if i > modded_model_grow.conversion_layer else modded_model_grow[i].out_channels\n",
    "        score = effective_rank(modded_model_grow.activations[str(i)])\n",
    "        to_add = max(score-int(0.95*max_rank), 0)\n",
    "        print(\"Layer {} score: {}/{}, neurons to add: {}\".format(i, score, max_rank, to_add))\n",
    "        modded_model_grow.grow(i, to_add, fanin_weights=\"iterative_orthogonalization\", \n",
    "                               optimizer=modded_optimizer_grow)\n",
    "    print(\"The grown model now has {} effective parameters.\".format(modded_model_grow.parameter_count(masked = True)))\n",
    "    print(\"Validation after growing: \", end = \"\")\n",
    "    test(modded_model_grow, val_loader, criterion)\n",
    "    train(modded_model_grow, train_loader, modded_optimizer_grow, criterion, epochs=2, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try masking neurons for simple grow-and-prune strategy. first doubling each layer's capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer 0 scores: mean 4.22, std 0.359, min 3.77, smallest 25% to mask: [3 6], random neurons to unmask: [ 8 10]\n",
      "\n",
      "Layer 1 scores: mean 18, std 0.597, min 17, smallest 25% to mask: [8 6 9 7], random neurons to unmask: [21 29 24 30]\n",
      "\n",
      "Layer 2 scores: mean 19.5, std 1.24, min 17.6, smallest 25% to mask: [ 0 12  8 13], random neurons to unmask: [27 31 24 25]\n",
      "\n",
      "Layer 3 scores: mean 7.35, std 0.45, min 6.65, smallest 25% to mask: [14 10 31  9 13 23 27 28], random neurons to unmask: [62 46 37 42 39 58 54 44]\n",
      "The masked model now has 30578 effective parameters.\n",
      "Validation after growing: Average loss: 0.0049, Accuracy: 4825/6000 (80.42%)\n",
      "Validation: Average loss: 0.0010, Accuracy: 5788/6000 (96.47%)\n",
      "Validation: Average loss: 0.0008, Accuracy: 5822/6000 (97.03%)\n",
      "\n",
      "Layer 0 scores: mean 4.28, std 0.495, min 3.52, smallest 25% to mask: [ 8 10], random neurons to unmask: [ 9 12]\n",
      "\n",
      "Layer 1 scores: mean 17.4, std 2.11, min 13.8, smallest 25% to mask: [29 30 24 21], random neurons to unmask: [20  7 22 25]\n",
      "\n",
      "Layer 2 scores: mean 19.6, std 3.47, min 13.8, smallest 25% to mask: [27 24 31 25], random neurons to unmask: [29 30  8 26]\n",
      "\n",
      "Layer 3 scores: mean 7.31, std 1.06, min 5.27, smallest 25% to mask: [46 54 37 58 42 39 44 62], random neurons to unmask: [28 57 40 41 51 45 55 63]\n",
      "The masked model now has 30581 effective parameters.\n",
      "Validation after growing: Average loss: 0.0012, Accuracy: 5743/6000 (95.72%)\n",
      "Validation: Average loss: 0.0008, Accuracy: 5829/6000 (97.15%)\n",
      "Validation: Average loss: 0.0007, Accuracy: 5855/6000 (97.58%)\n",
      "\n",
      "Layer 0 scores: mean 4.39, std 0.493, min 3.65, smallest 25% to mask: [12  9], random neurons to unmask: [13  3]\n",
      "\n",
      "Layer 1 scores: mean 17.8, std 1.94, min 14, smallest 25% to mask: [25 22 20  7], random neurons to unmask: [19 17 24  8]\n",
      "\n",
      "Layer 2 scores: mean 20.3, std 3.45, min 13.7, smallest 25% to mask: [26 29 30  8], random neurons to unmask: [12 13 28 18]\n",
      "\n",
      "Layer 3 scores: mean 7.39, std 1.18, min 4.95, smallest 25% to mask: [51 45 57 41 40 63 55 28], random neurons to unmask: [14 50  9 46 35 42 47 60]\n",
      "The masked model now has 30587 effective parameters.\n",
      "Validation after growing: Average loss: 0.0014, Accuracy: 5645/6000 (94.08%)\n",
      "Validation: Average loss: 0.0006, Accuracy: 5866/6000 (97.77%)\n",
      "Validation: Average loss: 0.0006, Accuracy: 5855/6000 (97.58%)\n",
      "\n",
      "Layer 0 scores: mean 4.44, std 0.543, min 3.54, smallest 25% to mask: [13  3], random neurons to unmask: [8 6]\n",
      "\n",
      "Layer 1 scores: mean 17.9, std 2.03, min 13.6, smallest 25% to mask: [19 24 17  8], random neurons to unmask: [28  7 20  6]\n",
      "\n",
      "Layer 2 scores: mean 20.8, std 3.11, min 14.2, smallest 25% to mask: [28 18 12 13], random neurons to unmask: [22 26 16 23]\n",
      "\n",
      "Layer 3 scores: mean 7.53, std 1.07, min 5.27, smallest 25% to mask: [46 60 35 42 47 50 14  9], random neurons to unmask: [28 62 56 23 41 53 37 51]\n",
      "The masked model now has 30590 effective parameters.\n",
      "Validation after growing: Average loss: 0.0021, Accuracy: 5520/6000 (92.00%)\n",
      "Validation: Average loss: 0.0006, Accuracy: 5867/6000 (97.78%)\n",
      "Validation: Average loss: 0.0006, Accuracy: 5843/6000 (97.38%)\n",
      "\n",
      "Layer 0 scores: mean 4.5, std 0.577, min 3.51, smallest 25% to mask: [8 6], random neurons to unmask: [14 15]\n",
      "\n",
      "Layer 1 scores: mean 18.2, std 1.8, min 13.9, smallest 25% to mask: [28 20  6  7], random neurons to unmask: [29 23 27 17]\n",
      "\n",
      "Layer 2 scores: mean 20.5, std 4.05, min 13.7, smallest 25% to mask: [26 22 23 16], random neurons to unmask: [17 31 21 24]\n",
      "\n",
      "Layer 3 scores: mean 7.58, std 1.14, min 4.94, smallest 25% to mask: [51 37 41 53 56 62 23 28], random neurons to unmask: [60 14 32 34 36 61 50 57]\n",
      "The masked model now has 30586 effective parameters.\n",
      "Validation after growing: Average loss: 0.0011, Accuracy: 5738/6000 (95.63%)\n",
      "Validation: Average loss: 0.0005, Accuracy: 5870/6000 (97.83%)\n",
      "Validation: Average loss: 0.0005, Accuracy: 5870/6000 (97.83%)\n"
     ]
    }
   ],
   "source": [
    "modded_model_masked = copy.deepcopy(model)\n",
    "modded_optimizer_masked = torch.optim.SGD(modded_model_masked.parameters(), lr=0.01)\n",
    "modded_optimizer_masked.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for i in range(len(modded_model_masked)-1):\n",
    "    neurons = modded_model_masked[i].out_features if i > modded_model_masked.conversion_layer else modded_model_masked[i].out_channels\n",
    "    modded_model_masked.grow(i, neurons, fanin_weights=\"kaiming\", fanout_weights=\"kaiming\", optimizer=modded_optimizer_masked)\n",
    "    modded_model_masked.mask(i, list(range(neurons, 2*neurons)))\n",
    "\n",
    "for iter in range(5):\n",
    "    for i in range(len(modded_model_masked)-1):\n",
    "        scores = weight_sum(modded_model_masked[i].get_weights())\n",
    "        print(\"Layer {} scores: mean {:.3g}, std {:.3g}, min {:.3g}, smallest 25% to mask:\".format(i, scores[scores != 0].mean(), scores[scores != 0].std(), scores[scores != 0].min()), end=\" \")\n",
    "        to_mask = np.argsort(scores.detach().numpy())[sum(scores == 0):sum(scores == 0)+int(0.25*sum(scores != 0))]\n",
    "        print(to_mask, end=\", \")\n",
    "        modded_model_masked.mask(i, to_mask)\n",
    "        to_unmask = np.argsort(scores.detach().numpy())[:sum(scores == 0)]\n",
    "        to_unmask = np.random.choice(to_unmask, size=len(to_mask), replace=False)\n",
    "        print(\"random neurons to unmask:\", to_unmask)\n",
    "        modded_model_masked.unmask(i, to_unmask, optimizer=modded_optimizer_masked)\n",
    "    print(\"The masked model now has {} effective parameters.\".format(modded_model_masked.parameter_count(masked = True)))\n",
    "    print(\"Validation after growing: \", end = \"\")\n",
    "    test(modded_model_masked, val_loader, criterion)\n",
    "    train(modded_model_masked, train_loader, modded_optimizer_masked, criterion, epochs=2, val_loader=val_loader, verbose=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
